{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4fb1f8d-d7de-45b1-a3b4-ad8df8297f76",
   "metadata": {},
   "source": [
    "# Spaceship Titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95848f4-8252-404f-be06-eff226f368a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c82c8284-78e3-4966-977f-00ab0143bf62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
    ")\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from skrebate import ReliefF\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import optuna\n",
    "from typing import Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "train_data = pd.read_csv('data/train.csv')\n",
    "test_data = pd.read_csv('data/test.csv')\n",
    "submission_data = pd.read_csv('data/sample_submission.csv')\n",
    "\n",
    "X = train_data.drop(columns='Transported')\n",
    "y = train_data['Transported']\n",
    "\n",
    "df = train_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feff4309-304c-42e9-9219-82248e68a20a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0f947b-841e-4bab-97e7-e9f4f53f2a27",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Visualizing Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1aeb3e-1163-4326-92f3-459cff1222ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(data=df, x='Transported', palette='viridis')\n",
    "plt.title('Distribution of Target Variable: Transported')\n",
    "plt.xlabel('Transported')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91a297b-679e-4f32-a6c1-d727c61a34c4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e59284-0e61-4c37-9324-ce2c33622146",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing Values:\\n\", missing_values)\n",
    "\n",
    "missing_percent = (df.isnull().sum() / len(df)) * 100\n",
    "plt.figure(figsize=(10, 6))\n",
    "missing_percent.sort_values(ascending=False).plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title('Percentage of Missing Values by Column')\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.xlabel('Columns')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da29010-fbc5-402f-a4d9-a259d1124c81",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Numerical Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f48f16-fe4a-47da-905a-f826071c485d",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "\n",
    "df[numerical_features].hist(bins=30, figsize=(10, 8), layout=(2, 3), color='skyblue', edgecolor='black')\n",
    "plt.suptitle('Distribution of Numerical Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, feature in enumerate(numerical_features, 1):\n",
    "    plt.subplot(2, 3, i)\n",
    "    sns.boxplot(data=df, x=feature, palette='coolwarm')\n",
    "    plt.title(f'{feature} Outlier Analysis')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff339495-46f3-415c-a3be-beedea26d06b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Categorical Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21260ce-0bcb-4d6d-9649-0f71db7a7084",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# in case of \"Cabin\" we can't understand from the graph\n",
    "categorical_features = ['HomePlanet', 'Destination', 'VIP']\n",
    "\n",
    "for feature in categorical_features:\n",
    "    print(f\"Unique values in {feature}: {df[feature].unique()}\")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, feature in enumerate(categorical_features, 1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    sns.countplot(data=df, x=feature, palette='viridis', order=df[feature].value_counts().index)\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff1d40-c28b-45ab-b3f0-36a1e63fc1b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Relationship Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735266ff-1c45-491f-8e0e-4c66721992da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numerical_features = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "correlation_matrix = df[numerical_features].corr()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", square=True, cbar=True)\n",
    "plt.title('Correlation Heatmap for Numerical Features')\n",
    "plt.show()\n",
    "\n",
    "sns.pairplot(df, vars=numerical_features, hue='Transported', palette='viridis', diag_kind='kde', corner=True)\n",
    "plt.suptitle('Pair Plot of Numerical Features with Transported', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeec108-aaac-4b50-a6e1-2aa9a4add020",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Feature Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d9b5dc-9ba2-4041-a82a-d92b531a323c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(data=df, x='CryoSleep', y='RoomService', palette='viridis')\n",
    "plt.title('RoomService Expenses by CryoSleep Status')\n",
    "plt.xlabel('CryoSleep')\n",
    "plt.ylabel('RoomService Expense')\n",
    "plt.show()\n",
    "\n",
    "df['TotalExpenses'] = df[numerical_features].sum(axis=1)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(data=df, x='CryoSleep', y='TotalExpenses', palette='coolwarm')\n",
    "plt.title('Total Expenses by CryoSleep Status')\n",
    "plt.xlabel('CryoSleep')\n",
    "plt.ylabel('Total Expenses')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83c639b-c27a-413a-b811-3136a7ff275d",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1639258a-50a8-443b-8bb3-d4370e392644",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[['Cabin_Deck', 'Cabin_Num', 'Cabin_Side']] = (\n",
    "    df['Cabin'].str.split('/', expand=True)\n",
    ")\n",
    "\n",
    "# Split PassengerId into components\n",
    "df[['Group', 'Group_Id']] = df['PassengerId'].str.split('_', expand=True)\n",
    "\n",
    "# Create TotalExpenses feature\n",
    "expense_cols = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "df['TotalExpenses'] = df[expense_cols].sum(axis=1)\n",
    "\n",
    "df = pd.get_dummies(df, columns=['Cabin_Deck'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c03493e5-2954-47ec-956a-4626c8901240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mz/90j2mwc116g4xxfj2_10f8b00000gn/T/ipykernel_1401/2853220896.py:10: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace(mappings)\n"
     ]
    }
   ],
   "source": [
    "mappings = {\n",
    "    'HomePlanet': {'Europa': 0, 'Earth': 1, 'Mars': 2},\n",
    "    'CryoSleep': {False: 0, True: 1},\n",
    "    'Destination': {'TRAPPIST-1e': 0, \"55 Cancri e\": 1, 'PSO J318.5-22': 2},\n",
    "    'VIP': {False: 0, True: 1},\n",
    "    'Cabin_Side': {'P': 0, 'S': 1},\n",
    "}\n",
    "df = df.replace(mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb9f3ff660a62c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data types\n",
    "df['Cabin_Num'] = df['Cabin_Num'].astype(float)\n",
    "df['Group_Id'] = df['Group_Id'].astype(float)\n",
    "df['Group'] = df['Group'].astype(float)\n",
    "\n",
    "cabin_cols = ['Cabin_Deck_A', 'Cabin_Deck_B', 'Cabin_Deck_C', 'Cabin_Deck_D',\n",
    "              'Cabin_Deck_E', 'Cabin_Deck_F', 'Cabin_Deck_G', 'Cabin_Deck_T']\n",
    "df[cabin_cols] = df[cabin_cols].astype(int)\n",
    "\n",
    "# Numerical columns\n",
    "num_cols = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall',\n",
    "            'Spa', 'VRDeck', 'Cabin_Num', 'Group_Id']\n",
    "df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n",
    "\n",
    "# Categorical columns\n",
    "cat_cols = [\n",
    "    'HomePlanet', 'CryoSleep', 'Destination', 'VIP', 'Cabin_Deck_A', 'Cabin_Deck_B', 'Cabin_Deck_C', 'Cabin_Deck_D',\n",
    "    'Cabin_Deck_E', 'Cabin_Deck_F', 'Cabin_Deck_G', 'Cabin_Deck_T', 'Cabin_Side', 'Group'\n",
    "]\n",
    "for col in cat_cols:\n",
    "    df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "# for training data drop 500 data points that we don't need\n",
    "df_temp = df[df['CryoSleep'] == 1.0]\n",
    "df = df.drop(df_temp[df_temp['Transported'] == 0].index)\n",
    "\n",
    "# Adjust CryoSleep based on spending\n",
    "spending_cols = ['FoodCourt', 'RoomService', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "\n",
    "# For passengers marked as cryosleep, ensure missing spending values become 0.\n",
    "cryo_mask = df['CryoSleep'] == 1.0\n",
    "df.loc[cryo_mask, spending_cols] = df.loc[cryo_mask, spending_cols].fillna(0)\n",
    "\n",
    "# Compute total spending across the five columns.\n",
    "spending_sum = df[spending_cols].sum(axis=1)\n",
    "\n",
    "# Reassign CryoSleep based on spending:\n",
    "# If total spending is 0, set CryoSleep to 1.0; otherwise, set it to 0.0.\n",
    "df.loc[spending_sum == 0, 'CryoSleep'] = 1.0\n",
    "df.loc[spending_sum != 0, 'CryoSleep'] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2880e4354f6c4c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['HasPaid'] = (df['RoomService'] + df['FoodCourt'] + df['ShoppingMall'] + df['Spa'] + df['VRDeck']) > 0\n",
    "df['Paid'] = df['RoomService'] + df['FoodCourt'] + df['ShoppingMall'] + df['Spa'] + df['VRDeck']\n",
    "df['HasPaid_RoomService'] = (df['RoomService']) > 0\n",
    "df['HasPaid_FoodCourt'] = (df['FoodCourt']) > 0\n",
    "df['HasPaid_ShoppingMall'] = (df['ShoppingMall']) > 0\n",
    "df['HasPaid_Spa'] = (df['Spa']) > 0\n",
    "df['HasPaid_VRDeck'] = (df['VRDeck']) > 0\n",
    "df['IsAdult'] = (df['Age']) > 17\n",
    "\n",
    "df['HasPaid'] = df['HasPaid'].astype(int)\n",
    "df['HasPaid_RoomService'] = df['HasPaid_RoomService'].astype(int)\n",
    "df['HasPaid_FoodCourt'] = df['HasPaid_FoodCourt'].astype(int)\n",
    "df['HasPaid_ShoppingMall'] = df['HasPaid_ShoppingMall'].astype(int)\n",
    "df['HasPaid_Spa'] = df['HasPaid_Spa'].astype(int)\n",
    "df['HasPaid_VRDeck'] = df['HasPaid_VRDeck'].astype(int)\n",
    "df['IsAdult'] = df['IsAdult'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da05f094-6110-46b0-b643-88ba26d2d52f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns=['PassengerId', 'Cabin', 'Name'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56cce0c4-c2fe-4a0c-ac14-b6f10b61d414",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Transported', axis=1)\n",
    "y = df['Transported'].astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2e09170-4d16-4685-836a-470f132a4d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'logistic_regression': LogisticRegression(),\n",
    "    'random_forest': RandomForestClassifier(),\n",
    "    'catboost': CatBoostClassifier(verbose=0),\n",
    "    'lightgbm': LGBMClassifier(n_estimators=40, learning_rate=0.1, max_depth=15),\n",
    "}\n",
    "\n",
    "def train_model(model_name: str, X: pd.DataFrame, y: pd.Series) -> dict:\n",
    "    \"\"\"Train specified model with cross-validation\"\"\"\n",
    "    model = models[model_name]\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "    model.fit(X, y)\n",
    "    return {\n",
    "        'model': model,\n",
    "        'cv_scores': scores,\n",
    "        'mean_score': np.mean(scores)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "916a402a-bd9a-4e75-b664-0d99e0c4117e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test: pd.DataFrame, y_test: pd.Series) -> dict:\n",
    "    \"\"\"Generate comprehensive evaluation metrics\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'classification_report': classification_report(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, y_proba) if y_proba is not None else None,\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8ca7fc2-aca8-4075-b3bf-afe7abdd800d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2788, number of negative: 2420\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001050 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2420\n",
      "[LightGBM] [Info] Number of data points in the train set: 5208, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.535330 -> initscore=0.141557\n",
      "[LightGBM] [Info] Start training from score 0.141557\n",
      "[LightGBM] [Info] Number of positive: 2789, number of negative: 2420\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000946 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2420\n",
      "[LightGBM] [Info] Number of data points in the train set: 5209, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.535419 -> initscore=0.141916\n",
      "[LightGBM] [Info] Start training from score 0.141916\n",
      "[LightGBM] [Info] Number of positive: 2789, number of negative: 2420\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000891 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2421\n",
      "[LightGBM] [Info] Number of data points in the train set: 5209, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.535419 -> initscore=0.141916\n",
      "[LightGBM] [Info] Start training from score 0.141916\n",
      "[LightGBM] [Info] Number of positive: 2789, number of negative: 2420\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001034 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2420\n",
      "[LightGBM] [Info] Number of data points in the train set: 5209, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.535419 -> initscore=0.141916\n",
      "[LightGBM] [Info] Start training from score 0.141916\n",
      "[LightGBM] [Info] Number of positive: 2789, number of negative: 2420\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001014 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2420\n",
      "[LightGBM] [Info] Number of data points in the train set: 5209, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.535419 -> initscore=0.141916\n",
      "[LightGBM] [Info] Start training from score 0.141916\n",
      "[LightGBM] [Info] Number of positive: 3486, number of negative: 3025\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001071 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2421\n",
      "[LightGBM] [Info] Number of data points in the train set: 6511, number of used features: 30\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.535402 -> initscore=0.141844\n",
      "[LightGBM] [Info] Start training from score 0.141844\n",
      "\n",
      "=== LOGISTIC_REGRESSION ===\n",
      "Mean CV Accuracy: 0.8424\n",
      "Test Accuracy: 0.8305\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.79      0.81       736\n",
      "           1       0.83      0.86      0.85       892\n",
      "\n",
      "    accuracy                           0.83      1628\n",
      "   macro avg       0.83      0.83      0.83      1628\n",
      "weighted avg       0.83      0.83      0.83      1628\n",
      "\n",
      "\n",
      "=== RANDOM_FOREST ===\n",
      "Mean CV Accuracy: 0.8498\n",
      "Test Accuracy: 0.8354\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.86      0.82       736\n",
      "           1       0.87      0.82      0.84       892\n",
      "\n",
      "    accuracy                           0.84      1628\n",
      "   macro avg       0.83      0.84      0.83      1628\n",
      "weighted avg       0.84      0.84      0.84      1628\n",
      "\n",
      "\n",
      "=== CATBOOST ===\n",
      "Mean CV Accuracy: 0.8632\n",
      "Test Accuracy: 0.8520\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.85      0.84       736\n",
      "           1       0.87      0.86      0.86       892\n",
      "\n",
      "    accuracy                           0.85      1628\n",
      "   macro avg       0.85      0.85      0.85      1628\n",
      "weighted avg       0.85      0.85      0.85      1628\n",
      "\n",
      "\n",
      "=== LIGHTGBM ===\n",
      "Mean CV Accuracy: 0.8572\n",
      "Test Accuracy: 0.8538\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.85      0.84       736\n",
      "           1       0.88      0.86      0.87       892\n",
      "\n",
      "    accuracy                           0.85      1628\n",
      "   macro avg       0.85      0.85      0.85      1628\n",
      "weighted avg       0.85      0.85      0.85      1628\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_results = {}\n",
    "for model_name in ['logistic_regression', 'random_forest', 'catboost', 'lightgbm']:\n",
    "    result = train_model(model_name, X_train, y_train)\n",
    "    evaluation = evaluate_model(result['model'], X_test, y_test)\n",
    "    model_results[model_name] = {**result, **evaluation}\n",
    "\n",
    "for model_name, metrics in model_results.items():\n",
    "    print(f\"\\n=== {model_name.upper()} ===\")\n",
    "    print(f\"Mean CV Accuracy: {metrics['mean_score']:.4f}\")\n",
    "    print(f\"Test Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(metrics['classification_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c4160aa-3745-41ad-a33f-3d57cd81d516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HomePlanet</th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Age</th>\n",
       "      <th>VIP</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "      <th>...</th>\n",
       "      <th>Cabin_Deck_G</th>\n",
       "      <th>Cabin_Deck_T</th>\n",
       "      <th>HasPaid</th>\n",
       "      <th>Paid</th>\n",
       "      <th>HasPaid_RoomService</th>\n",
       "      <th>HasPaid_FoodCourt</th>\n",
       "      <th>HasPaid_ShoppingMall</th>\n",
       "      <th>HasPaid_Spa</th>\n",
       "      <th>HasPaid_VRDeck</th>\n",
       "      <th>IsAdult</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>736.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>3576.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6715.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10383.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>3329.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5176.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>303.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1091.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>483.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>291.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>774.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   HomePlanet  CryoSleep  Destination   Age  VIP  RoomService  FoodCourt  \\\n",
       "1         1.0        0.0          0.0  24.0  0.0        109.0        9.0   \n",
       "2         0.0        0.0          0.0  58.0  1.0         43.0     3576.0   \n",
       "3         0.0        0.0          0.0  33.0  0.0          0.0     1283.0   \n",
       "4         1.0        0.0          0.0  16.0  0.0        303.0       70.0   \n",
       "5         1.0        0.0          2.0  44.0  0.0          0.0      483.0   \n",
       "\n",
       "   ShoppingMall     Spa  VRDeck  ...  Cabin_Deck_G  Cabin_Deck_T  HasPaid  \\\n",
       "1          25.0   549.0    44.0  ...             0             0        1   \n",
       "2           0.0  6715.0    49.0  ...             0             0        1   \n",
       "3         371.0  3329.0   193.0  ...             0             0        1   \n",
       "4         151.0   565.0     2.0  ...             0             0        1   \n",
       "5           0.0   291.0     0.0  ...             0             0        1   \n",
       "\n",
       "      Paid  HasPaid_RoomService  HasPaid_FoodCourt  HasPaid_ShoppingMall  \\\n",
       "1    736.0                    1                  1                     1   \n",
       "2  10383.0                    1                  1                     0   \n",
       "3   5176.0                    0                  1                     1   \n",
       "4   1091.0                    1                  1                     1   \n",
       "5    774.0                    0                  1                     0   \n",
       "\n",
       "   HasPaid_Spa  HasPaid_VRDeck  IsAdult  \n",
       "1            1               1        1  \n",
       "2            1               1        1  \n",
       "3            1               1        1  \n",
       "4            1               1        0  \n",
       "5            1               0        1  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d89f2b5-1a84-4cf4-bd99-b1fe957ab89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "        'class_weight': trial.suggest_categorical('class_weight', [None, 'balanced'])\n",
    "    }\n",
    "    model = RandomForestClassifier(**params, random_state=42)\n",
    "    return cross_val_score(model, X, y, cv=3, scoring='accuracy').mean()\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "self.best_model = RandomForestClassifier(**study.best_params)\n",
    "print(f\"Best parameters from optimization: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7683efb-d993-47e5-9c0c-af82b8c5ec3e",
   "metadata": {},
   "source": [
    "### Check if the data is imbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449523e6-c4f4-4831-bb92-63d4d656ede2",
   "metadata": {},
   "source": [
    "### Scale and Split Data into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b92a752-1660-4208-a4f5-cd7544233df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'logistic_regression': LogisticRegression(),\n",
    "    'random_forest': RandomForestClassifier(),\n",
    "    'catboost': CatBoostClassifier(verbose=0),\n",
    "    'lightgdm': LGBMClassifier(n_estimators=40, learning_rate=0.1, max_depth=15),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321a25741e3efa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(self, model_name: str, X: pd.DataFrame, y: pd.Series) -> dict:\n",
    "    \"\"\"Train specified model with cross-validation\"\"\"\n",
    "    model = self.models[model_name]\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "    model.fit(X, y)\n",
    "    return {\n",
    "        'model': model,\n",
    "        'cv_scores': scores,\n",
    "        'mean_score': np.mean(scores)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592c6a7a6a72b982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_hyperparameters(self, X: pd.DataFrame, y: pd.Series) -> dict:\n",
    "    \"\"\"Optimize Random Forest parameters using Optuna\"\"\"\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "            'class_weight': trial.suggest_categorical('class_weight', [None, 'balanced'])\n",
    "        }\n",
    "        model = RandomForestClassifier(**params, random_state=self.config.random_state)\n",
    "        return cross_val_score(model, X, y, cv=3, scoring='accuracy').mean()\n",
    "\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=100)\n",
    "    self.best_model = RandomForestClassifier(**study.best_params)\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5f7a2ef0498f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test: pd.DataFrame, y_test: pd.Series) -> dict:\n",
    "    \"\"\"Generate comprehensive evaluation metrics\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'classification_report': classification_report(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, y_proba) if y_proba is not None else None,\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae57a2d41ab9ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(self, processed_df: pd.DataFrame):\n",
    "    \"\"\"Original training logic with splits\"\"\"\n",
    "    # Prepare data splits (already done in run())\n",
    "    # Feature selection\n",
    "    X_train_selected = self.model_trainer.feature_selection(self.X_train, self.y_train)\n",
    "\n",
    "    # Hyperparameter optimization\n",
    "    self.best_params = self.model_trainer.optimize_hyperparameters(X_train_selected, self.y_train)\n",
    "\n",
    "    # Store best params for later use\n",
    "    self.model_trainer.best_model = RandomForestClassifier(**self.best_params)\n",
    "    self.model_trainer.best_model.fit(X_train_selected, self.y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf22bb25f2b4c7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_splits(self, df: pd.DataFrame):\n",
    "    \"\"\"Create train/test splits\"\"\"\n",
    "    X = df.drop('Transported', axis=1)\n",
    "    y = df['Transported'].astype(int)\n",
    "    self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "        X, y, test_size=self.config.test_size, random_state=self.config.random_state\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed372efb5a6bc2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = data_processor.preprocess_data(train_df)\n",
    "prepare_splits(processed_df)\n",
    "train_and_evaluate(processed_df)\n",
    "\n",
    "model_results = {}\n",
    "for model_name in models.keys():\n",
    "    result = train_model(model_name, X_train, y_train)\n",
    "    evaluation = evaluate_model(result['model'], X_test, y_test)\n",
    "    model_results[model_name] = {**result, **evaluation}\n",
    "\n",
    "# Hyperparameter optimization\n",
    "best_params = optimize_hyperparameters(X_train, y_train)\n",
    "print(f\"Best parameters from optimization: {best_params}\")\n",
    "\n",
    "model_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
